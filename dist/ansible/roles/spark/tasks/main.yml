---
    - name: Insert hostnames and ip in /etc/hosts file
      blockinfile:
        dest: /etc/hosts
        block: "{{ lookup('template', 'hosts.j2') }}"
        marker: "# {mark} ANSIBLE MANAGED BLOCK FOR SPARK"

    - name: Install java 11
      ansible.builtin.package:
        name:
        - java-11-openjdk-headless
        state: latest

    - name: Create Spark user
      user:
        name: "{{ spark_user }}"
        group: "{{ org_group }}"
        shell: /bin/bash
        create_home: yes
      register: spark_user_registered

    - name: ensure ivy cache directory exists
      file:
        path: "{{ spark_user_registered.home }}/.ivy/cache"
        state: directory
        owner: "{{ spark_user }}"
        group: "{{ org_group }}"
        mode: '770'

    - name: ensure new spark directory exists
      file:
        path: "{{ spark_home }}NEWSPARK"
        state: directory
        owner: "{{ spark_user }}"
        group: "{{ org_group }}"
        mode: '770'

    - name: Extract Spark tarball
      unarchive:
        src: "{{ spark_airgap_archive }}"
        dest: "{{ spark_home }}NEWSPARK"
        remote_src: no
        owner: "{{ spark_user }}"
        group: "{{ org_group }}"
        creates: "{{ spark_home }}/conf"

    - name: Move foo to bar
      command: mv {{ spark_home }}NEWSPARK/{{spark_airgap_archive[:-4]|basename}} {{ spark_home }}
      args:
        creates: "{{ spark_home }}/conf"

    - name: Delete content & directory
      ansible.builtin.file:
        state: absent
        path: "{{ spark_home }}NEWSPARK"

    - name: ensure spark directories exists
      file:
        path: "{{ item }}"
        state: directory
        owner: "{{ spark_user }}"
        group: "{{ org_group }}"
        mode: '770'
      loop:
        - "{{ spark_log_dir }}"
        - "{{ spark_local_dir }}"
        - "{{ spark_eventLog_dir }}"
        - "{{ spark_worker_dir }}"

    - name: Create spark-env.sh
      template:
        src: "spark-env.sh.j2"
        dest: "{{ spark_home }}/conf/spark-env.sh"
        owner: "{{ spark_user }}"
        group: "{{ org_group }}"
        mode: '770'


    - name: Create spark-defaults.conf
      template:
        src: "spark-defaults.conf.j2"
        dest: "{{ spark_home }}/conf/spark-defaults.conf"
        owner: "{{ spark_user }}"
        group: "{{ org_group }}"
        mode: '770'

    - name: Configure Spark to point to the master node
      when: inventory_hostname in groups['spark_workers']
      lineinfile:
        path: "{{ spark_home }}/conf/spark-defaults.conf"
        line: 'spark.master {{ spark_master_url }}'
        create: yes
        owner: "{{ spark_user }}"
        group: "{{ org_group }}"
        mode: '770'


    - name: Create system systemd spark master service file
      when: inventory_hostname in groups['spark_master']
      template:
        src: "spark-master.service.j2"
        dest: /etc/systemd/system/spark-master.service

    - name: Create system systemd spark slave service file
      when: inventory_hostname in groups['spark_workers']
      template:
        src: "spark-slave.service.j2"
        dest: /etc/systemd/system/spark-slave.service

    - name: Start Spark master on master node
      when: inventory_hostname in groups['spark_master']
      systemd:
        name: spark-master
        state: started
        enabled: true

    - name: Start Spark worker on worker nodes
      when: inventory_hostname in groups['spark_workers']
      systemd:
        name: spark-slave
        state: started
        enabled: true

